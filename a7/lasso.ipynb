{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce4ec8",
   "metadata": {},
   "source": [
    "Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia red√©marrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
    "\n",
    "Izay misy hoe `YOUR CODE HERE` na \"YOUR ANSWER HERE\" ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581bb28",
   "metadata": {},
   "source": [
    "## References\n",
    "Eto ilay references rehetra no apetraka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678e059",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/code/residentmario/soft-thresholding-with-lasso-regression/notebook\n",
    "# https://xavierbourretsicotte.github.io/lasso_implementation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab42c33c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1e2e40465642c4fdf199b183cad1aca",
     "grade": false,
     "grade_id": "cell-38b4529793385ad1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7abcf9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fece078db4194ca23bff4bd477484c9f",
     "grade": false,
     "grade_id": "cell-2d8d2723a816a3da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6b011f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a58f8c2b00062e871c025fe26fd9f14",
     "grade": false,
     "grade_id": "cell-1528a28f467d90c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss_vectorized(w, b, X, y):\n",
    "    \"\"\"\n",
    "    MSE loss function WITHOUT FOR LOOPs , NO REGULARIZATION\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    y_pred = X.dot(w) + b\n",
    "    \n",
    "    # loss \n",
    "    loss = (1.0 / X.shape[0]) * ((y - y_pred) ** 2).sum()\n",
    "    \n",
    "    # gradient with respect to weights\n",
    "    dw = - (2.0 / X.shape[0]) * X.T.dot(y - y_pred)\n",
    "    \n",
    "    # gradient with respect to bias\n",
    "    db = - (2.0 / X.shape[0]) * ((y - y_pred)).sum()\n",
    "        \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f698dab4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a555de4acfff6e4b6b10deedfcec8a97",
     "grade": false,
     "grade_id": "cell-0317c2a47220a71f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def soft_threshold(x, delta):\n",
    "    # YOUR CODE HERE\n",
    "    if x > delta:\n",
    "        return x - delta\n",
    "    elif x < (- delta):\n",
    "        return x + delta\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3f35e",
   "metadata": {},
   "source": [
    "# Lasso Subgradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fdac6f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13377ad3c38d687f1097ea78e67e00d7",
     "grade": false,
     "grade_id": "cell-1f9c055ff5b25c0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def l1_subgradient(w):\n",
    "    \"\"\"\n",
    "    Subgradient of the L1 loss\n",
    "    \"\"\"\n",
    "    dw = np.zeros_like(w)\n",
    "    # YOUR CODE HERE\n",
    "    dw = np.sign(w)\n",
    "    return dw\n",
    "    \n",
    "\n",
    "def lasso_subgradient_mse_loss_vectorized(w, b, X, y, alpha):\n",
    "    \"\"\"\n",
    "    MSE loss function adding the subgradient for w\n",
    "    \"\"\"\n",
    "    loss, dw, db = mse_loss_vectorized(w, b, X, y)\n",
    "    # Add the subgradient to dw\n",
    "    # YOUR CODE HERE\n",
    "    dw += alpha * l1_subgradient(w)\n",
    "    return loss, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfaa3d7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f147544f1239cc3a8b31c9de6b9df0d",
     "grade": false,
     "grade_id": "cell-10e416072a8b14f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LassolSubgradientDescent():\n",
    "    def __init__(self,  alpha=0.1):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)                                                                                          \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            # YOUR CODE HERE\n",
    "            choices = np.random.choice(N, batch_size, replace=False)\n",
    "            \n",
    "            X_batch = X[choices, :]\n",
    "            y_batch = y[choices]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w using the gradient and the learning rate.  \n",
    "            # YOUR CODE HERE\n",
    "            self.w -= learning_rate * dw\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = X.dot(self.w) + self.b\n",
    "        return y_pred\n",
    "        \n",
    "    def loss(self, X_batch, y_batch):\n",
    "        return lasso_subgradient_mse_loss_vectorized(self.w, self.b, X_batch, y_batch, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e20f18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afc1e003d52538e82629ca2282c819ba",
     "grade": true,
     "grade_id": "cell-8a6c6c3fea10a22e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200000: loss 28079.511818\n",
      "iteration 10000 / 200000: loss 3122.122676\n",
      "iteration 20000 / 200000: loss 3341.384934\n",
      "iteration 30000 / 200000: loss 3256.374490\n",
      "iteration 40000 / 200000: loss 3186.214769\n",
      "iteration 50000 / 200000: loss 3029.594546\n",
      "iteration 60000 / 200000: loss 3014.983863\n",
      "iteration 70000 / 200000: loss 3296.357668\n",
      "iteration 80000 / 200000: loss 2852.318491\n",
      "iteration 90000 / 200000: loss 2658.461736\n",
      "iteration 100000 / 200000: loss 2869.821959\n",
      "iteration 110000 / 200000: loss 3143.727550\n",
      "iteration 120000 / 200000: loss 2837.094063\n",
      "iteration 130000 / 200000: loss 2965.729453\n",
      "iteration 140000 / 200000: loss 3006.978603\n",
      "iteration 150000 / 200000: loss 3003.509894\n",
      "iteration 160000 / 200000: loss 2788.598590\n",
      "iteration 170000 / 200000: loss 2950.872155\n",
      "iteration 180000 / 200000: loss 3115.911219\n",
      "iteration 190000 / 200000: loss 2686.326729\n",
      "MSE scikit-learn: 2912.527492603619\n",
      "MSE Coordinate descent model : 2888.7736561738557\n"
     ]
    }
   ],
   "source": [
    "model = LassolSubgradientDescent(alpha=0.1)\n",
    "model.train(X, y, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X)\n",
    "mse = mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=0.1, fit_intercept=True)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9e02f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5014aa1adfb704c0b9a8b597121bed85",
     "grade": true,
     "grade_id": "cell-67638b8b82d2ee79",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200000: loss 30538.783813\n",
      "iteration 10000 / 200000: loss 4057.922724\n",
      "iteration 20000 / 200000: loss 3961.010640\n",
      "iteration 30000 / 200000: loss 3751.226429\n",
      "iteration 40000 / 200000: loss 3964.958744\n",
      "iteration 50000 / 200000: loss 3719.298162\n",
      "iteration 60000 / 200000: loss 3752.850105\n",
      "iteration 70000 / 200000: loss 3520.351616\n",
      "iteration 80000 / 200000: loss 3609.868670\n",
      "iteration 90000 / 200000: loss 3486.128004\n",
      "iteration 100000 / 200000: loss 4240.151209\n",
      "iteration 110000 / 200000: loss 3560.023693\n",
      "iteration 120000 / 200000: loss 3783.886059\n",
      "iteration 130000 / 200000: loss 3976.368304\n",
      "iteration 140000 / 200000: loss 3572.314639\n",
      "iteration 150000 / 200000: loss 4011.673369\n",
      "iteration 160000 / 200000: loss 3680.816407\n",
      "iteration 170000 / 200000: loss 3251.212015\n",
      "iteration 180000 / 200000: loss 3710.123773\n",
      "iteration 190000 / 200000: loss 3930.049429\n",
      "MSE scikit-learn: 5650.290772564547\n",
      "MSE Coordinate descent model : 3810.4839627301453\n"
     ]
    }
   ],
   "source": [
    "model = LassolSubgradientDescent(alpha=2)\n",
    "model.train(X, y, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X)\n",
    "mse = mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=2, fit_intercept=True)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec4298",
   "metadata": {},
   "source": [
    "# Lasso Proximal Gradient Descent (iterative soft thresholding algorithm or ISTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea93876",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b58bd5f2fe2a364683285cec3dae2e7",
     "grade": false,
     "grade_id": "cell-7a3567bcf30b2f18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LassoProximalGradientDescent():\n",
    "    def __init__(self,  alpha=0.1):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)                                                                                          \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            # YOUR CODE HERE\n",
    "            choices = np.random.choice(N, batch_size, replace=False)\n",
    "            \n",
    "            X_batch = X[choices, :]\n",
    "            y_batch = y[choices]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w using the gradient and the learning rate.  \n",
    "            # PROJECT THE GRADIENT FOR w USING soft_threshold\n",
    "            # YOUR CODE HERE\n",
    "            for i in range(d):\n",
    "                self.w[i] = soft_threshold(self.w[i] - learning_rate * dw[i], self.alpha * learning_rate)\n",
    "\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = X.dot(self.w) + self.b\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def loss(self, X_batch, y_batch):\n",
    "        return mse_loss_vectorized(self.w, self.b, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9388e722",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc9f465d5c28afa8d682d76852df6218",
     "grade": true,
     "grade_id": "cell-e12e8a6bae8e7b5c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200000: loss 28303.829308\n",
      "iteration 10000 / 200000: loss 25121.938963\n",
      "iteration 20000 / 200000: loss 25208.246510\n",
      "iteration 30000 / 200000: loss 28119.308063\n",
      "iteration 40000 / 200000: loss 26199.436010\n",
      "iteration 50000 / 200000: loss 25027.036846\n",
      "iteration 60000 / 200000: loss 25689.155830\n",
      "iteration 70000 / 200000: loss 26142.117494\n",
      "iteration 80000 / 200000: loss 25037.314321\n",
      "iteration 90000 / 200000: loss 26581.107467\n",
      "iteration 100000 / 200000: loss 24789.983179\n",
      "iteration 110000 / 200000: loss 26883.466918\n",
      "iteration 120000 / 200000: loss 26101.222019\n",
      "iteration 130000 / 200000: loss 25759.210875\n",
      "iteration 140000 / 200000: loss 26050.222921\n",
      "iteration 150000 / 200000: loss 25082.949517\n",
      "iteration 160000 / 200000: loss 26760.403039\n",
      "iteration 170000 / 200000: loss 27480.324144\n",
      "iteration 180000 / 200000: loss 25656.546536\n",
      "iteration 190000 / 200000: loss 24801.715024\n",
      "MSE scikit-learn: 2912.527492603619\n",
      "MSE Coordinate descent model : 26033.2230135236\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4266/3840544519.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE scikit-learn:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE Coordinate descent model :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msk_mse\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LassoProximalGradientDescent(alpha=0.1)\n",
    "model.train(X, y, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X)\n",
    "mse= mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=0.1, fit_intercept=True)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7726b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "492b972e858ad56355406357894884a3",
     "grade": true,
     "grade_id": "cell-a25ecff481c69c72",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200000: loss 31065.844852\n",
      "iteration 10000 / 200000: loss 28263.413042\n",
      "iteration 20000 / 200000: loss 24970.214063\n",
      "iteration 30000 / 200000: loss 26195.830894\n",
      "iteration 40000 / 200000: loss 26470.020557\n",
      "iteration 50000 / 200000: loss 28455.296849\n",
      "iteration 60000 / 200000: loss 26030.475868\n",
      "iteration 70000 / 200000: loss 26395.213199\n",
      "iteration 80000 / 200000: loss 26339.430150\n",
      "iteration 90000 / 200000: loss 26609.328456\n",
      "iteration 100000 / 200000: loss 25704.796907\n",
      "iteration 110000 / 200000: loss 26563.886070\n",
      "iteration 120000 / 200000: loss 27837.891950\n",
      "iteration 130000 / 200000: loss 24823.387382\n",
      "iteration 140000 / 200000: loss 27741.275542\n",
      "iteration 150000 / 200000: loss 25468.608732\n",
      "iteration 160000 / 200000: loss 26058.032842\n",
      "iteration 170000 / 200000: loss 27505.722605\n",
      "iteration 180000 / 200000: loss 25080.703934\n",
      "iteration 190000 / 200000: loss 27256.150303\n",
      "MSE scikit-learn: 5650.290772564547\n",
      "MSE Coordinate descent model : 26951.47898377782\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4266/2004310501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE scikit-learn:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE Coordinate descent model :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msk_mse\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LassoProximalGradientDescent(alpha=2)\n",
    "model.train(X, y, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X)\n",
    "mse= mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=2, fit_intercept=True)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd57e0",
   "metadata": {},
   "source": [
    "# Lasso Projected Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0645e8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa6df72eb9b273f99ec243c076780b62",
     "grade": false,
     "grade_id": "cell-f3bad6c8edcb61e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LassoProjectedGradientDescent():\n",
    "    def __init__(self,  alpha=0.1):\n",
    "        self.w_p = None\n",
    "        self.w_n = None\n",
    "        self.b = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w_p is None: # Initialization\n",
    "            self.w_p = 0.001 * np.random.randn(d)\n",
    "            self.w_n = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)                                                                                          \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            # YOUR CODE HERE\n",
    "            choices = np.random.choice(N, batch_size, replace=False)\n",
    "            \n",
    "            X_batch = X[choices, :]\n",
    "            y_batch = y[choices]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w using the gradient and the learning rate.  \n",
    "            # Project for w_p and w_n\n",
    "            # YOUR CODE HERE\n",
    "            w = self.w_p - self.w_n\n",
    "            grad = - X_batch.T.dot(y_batch - (X_batch.dot(w)))\n",
    "    \n",
    "            grad_wn = -grad + self.alpha * l1_subgradient(w)\n",
    "            grad_wp = grad + self.alpha * l1_subgradient(w)\n",
    "            \n",
    "            self.w_p -= learning_rate * grad_wp\n",
    "            self.w_n -= learning_rate * grad_wn\n",
    "            \n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = X.dot(self.w_p - self.w_n) + self.b\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch):\n",
    "        # YOUR CODE HERE\n",
    "        return mse_loss_vectorized((self.w_p-self.w_n), self.b, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40763f91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61b4a80e91fed67c73a22e3f23a7be0b",
     "grade": true,
     "grade_id": "cell-ab833ef2fbf36bc3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200000: loss 31644.673352\n",
      "iteration 10000 / 200000: loss 3158.049157\n",
      "iteration 20000 / 200000: loss 3005.114620\n",
      "iteration 30000 / 200000: loss 2731.196819\n",
      "iteration 40000 / 200000: loss 2958.091757\n",
      "iteration 50000 / 200000: loss 2771.784644\n",
      "iteration 60000 / 200000: loss 2863.866436\n",
      "iteration 70000 / 200000: loss 2689.568956\n",
      "iteration 80000 / 200000: loss 2880.072004\n",
      "iteration 90000 / 200000: loss 2662.905580\n",
      "iteration 100000 / 200000: loss 2670.338545\n",
      "iteration 110000 / 200000: loss 3397.263616\n",
      "iteration 120000 / 200000: loss 2758.700074\n",
      "iteration 130000 / 200000: loss 2565.545559\n",
      "iteration 140000 / 200000: loss 2804.618970\n",
      "iteration 150000 / 200000: loss 3047.074908\n",
      "iteration 160000 / 200000: loss 2993.571979\n",
      "iteration 170000 / 200000: loss 2896.623305\n",
      "iteration 180000 / 200000: loss 3352.199130\n",
      "iteration 190000 / 200000: loss 2855.049434\n",
      "MSE scikit-learn: 2912.527492603619\n",
      "MSE Coordinate descent model : 2861.3285262785125\n"
     ]
    }
   ],
   "source": [
    "model = LassoProjectedGradientDescent(alpha=0.1)\n",
    "model.train(X, y, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X)\n",
    "mse= mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=0.1, fit_intercept=True)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb125d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5302dfe9ed74a0adcd0f27c24ea15b6c",
     "grade": true,
     "grade_id": "cell-d4d95fe8b23988a9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200000: loss 30385.473770\n",
      "iteration 10000 / 200000: loss 2804.456420\n",
      "iteration 20000 / 200000: loss 2878.172622\n",
      "iteration 30000 / 200000: loss 2739.772481\n",
      "iteration 40000 / 200000: loss 2894.393529\n",
      "iteration 50000 / 200000: loss 3018.068512\n",
      "iteration 60000 / 200000: loss 2924.631573\n",
      "iteration 70000 / 200000: loss 3000.226108\n",
      "iteration 80000 / 200000: loss 2801.622795\n",
      "iteration 90000 / 200000: loss 2974.558523\n",
      "iteration 100000 / 200000: loss 2651.998150\n",
      "iteration 110000 / 200000: loss 3121.434183\n",
      "iteration 120000 / 200000: loss 2457.852914\n",
      "iteration 130000 / 200000: loss 2838.367025\n",
      "iteration 140000 / 200000: loss 2764.479709\n",
      "iteration 150000 / 200000: loss 2535.280948\n",
      "iteration 160000 / 200000: loss 2979.872411\n",
      "iteration 170000 / 200000: loss 2949.383039\n",
      "iteration 180000 / 200000: loss 2919.857808\n",
      "iteration 190000 / 200000: loss 2813.317903\n",
      "MSE scikit-learn: 5650.290772564547\n",
      "MSE Coordinate descent model : 2863.4221601643208\n"
     ]
    }
   ],
   "source": [
    "model = LassoProjectedGradientDescent(alpha=2)\n",
    "model.train(X, y, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X)\n",
    "mse= mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=2, fit_intercept=True)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403ea0",
   "metadata": {},
   "source": [
    "# Lasso Coordinate Descent (no intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5039af1c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "700961f62d0be50b4d8a56693d51d147",
     "grade": false,
     "grade_id": "cell-485e52c0efd4f4a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LassoCoordinateDescent():\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.w = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self, X, y, num_iters=1000):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #normalizing X in case it was not done before\n",
    "        X = X / (np.linalg.norm(X, axis = 0)) \n",
    "        \n",
    "        # Initialization\n",
    "        self.w = np.ones((d,1))\n",
    "            \n",
    "        soft = np.vectorize(soft_threshold)\n",
    "        \n",
    "        #Looping until max number of iterations\n",
    "        for i in range(num_iters): \n",
    "\n",
    "            #Looping through each coordinate\n",
    "            for j in range(d):\n",
    "\n",
    "                #Vectorized implementation\n",
    "                X_j = X[:,j].reshape(-1,1)\n",
    "                y_pred = (X @ self.w).reshape(-1)\n",
    "                rho = X_j.T @ (y - y_pred.squeeze() + (self.w[j] * X_j).squeeze())\n",
    "                # no fit_intercept\n",
    "                self.w[j] = soft_threshold(rho, self.alpha)\n",
    "                \n",
    "        self.w.flatten()\n",
    "\n",
    "    def predict(self, X): \n",
    "        # YOUR CODE HERE\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccdee31b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "980c457daa4b6be9e9ce771fa7acbb65",
     "grade": true,
     "grade_id": "cell-a773a38c72b967e8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE scikit-learn: 26057.124496145752\n",
      "MSE Coordinate descent model : 26004.303657948534\n"
     ]
    }
   ],
   "source": [
    "model = LassoCoordinateDescent(alpha=0.1)\n",
    "model.train(X, y)\n",
    "pred = model.predict(X)\n",
    "mse= mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=0.1, fit_intercept=False)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "396f7431",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af63575c6cf174f251d81ec478828cc3",
     "grade": true,
     "grade_id": "cell-df22a0ef3e94d5fe",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE scikit-learn: 28794.887776106683\n",
      "MSE Coordinate descent model : 26006.422489676806\n"
     ]
    }
   ],
   "source": [
    "model = LassoCoordinateDescent(alpha=2)\n",
    "model.train(X, y)\n",
    "pred = model.predict(X)\n",
    "mse= mean_squared_error(pred, y)\n",
    "\n",
    "sk_model = Lasso(alpha=2, fit_intercept=False)\n",
    "sk_model.fit(X, y)\n",
    "sk_pred = sk_model.predict(X)\n",
    "sk_mse = mean_squared_error(sk_pred, y)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed468d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
